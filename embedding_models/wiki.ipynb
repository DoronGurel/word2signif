{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wiki corpus - model training\n",
    "collecting and processing text from wikipedia data dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os.path\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import translation_matrix\n",
    "import gensim\n",
    "import sys\n",
    "from gensim.corpora import WikiCorpus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: randomize input order\n",
    "for our experiment we would like to randomize the input text stream. <br>\n",
    "we will take the first 100,000 articles and put each article to a seperate file. <br>\n",
    "after this we could access each article directly, and we can input them in a randow order to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdirectory = 'first100k'\n",
    "out_f = 'enwiki_processed.txt'\n",
    "out_f_shuffle = 'article_{}.txt'\n",
    "\n",
    "\n",
    "corpus_file = open(out_f,'r')\n",
    "for article_num in range(100000):\n",
    "    output_shuffle_file = open(os.path.join(subdirectory,out_f_shuffle.format(article_num)), 'a+')\n",
    "    article = corpus_file.readline()\n",
    "    output_shuffle_file.write(article + '\\n')\n",
    "    output_shuffle_file.close()\n",
    "corpus_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to not fill our RAM and crash our computer, let's first define an itterator for wiki articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "        self.file_list = [fname for fname in os.listdir(dirname) if fname.endswith('.txt')]\n",
    "        random.shuffle(self.file_list)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for article in self.file_list:\n",
    "            for line in open(os.path.join(self.dirname, article)):\n",
    "                yield line.split()\n",
    "                \n",
    "    def reshuffle(self):\n",
    "        random.shuffle(self.file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: train models \n",
    "ok! now we can train the word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Part 1/4 [==================================================] 100.0% 1950.0/1950.0MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 09:47:39,253:INFO:Part 1/4 downloaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Part 2/4 [==================================================] 100.0% 1950.0/1950.0MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 09:54:39,090:INFO:Part 2/4 downloaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Part 3/4 [==================================================] 100.0% 1950.0/1950.0MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 10:02:09,340:INFO:Part 3/4 downloaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Part 4/4 [==================================================] 100.0% 364.2/364.2MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-27 10:03:30,081:INFO:Part 4/4 downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wiki_corpus = api.load('wiki-english-20171001')\n",
    "#corpus = api.load('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Dataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-91ac1d358155>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwiki_corpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'Dataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "wiki_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "sentences = MySentences(subdirectory) # a memory-friendly iterator\n",
    "model = gensim.models.Word2Vec(min_count=20, size=100, workers=4)\n",
    "model.build_vocab(sentences)\n",
    "model.save('empty_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now for the real training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136584684"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdirectory = 'first100k'\n",
    "sentences = MySentences(subdirectory) # a memory-friendly iterator\n",
    "\n",
    "for i in range(1,21):\n",
    "    logging.info('starting model {}'.format(i))\n",
    "    sentences.reshuffle()\n",
    "    model = gensim.models.Word2Vec.load(\"empty_model\")\n",
    "    logging.info('model training {}'.format(i))\n",
    "    model.train(sentences, total_examples=model.corpus_count, epochs=1)\n",
    "    logging.info('saving model {}'.format(i))\n",
    "    word_vectors = model.wv\n",
    "    fname = 'WV_dir/WV{}.kv'\n",
    "    word_vectors.save(fname.format(i))\n",
    "    logging.info('finished model {}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's check our model makes some intuitive sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spain', 0.7594519257545471),\n",
       " ('belgium', 0.7488405704498291),\n",
       " ('bordeaux', 0.7018789052963257),\n",
       " ('portugal', 0.6989240646362305),\n",
       " ('italy', 0.6878964900970459),\n",
       " ('provence', 0.6737589836120605)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = [\"france\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "word_vectors = model.wv\n",
    "fname = 'WV1.kv'\n",
    "word_vectors.save(fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
